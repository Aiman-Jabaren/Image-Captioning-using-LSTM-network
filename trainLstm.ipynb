{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder_new import *\n",
    "from encoder import *\n",
    "from data_loader import *\n",
    "import pickle\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import torchvision.transforms as tf\n",
    "import json\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate(val_loader, encoder, decoder, criterion, maxSeqLen, vocablen, use_gpu = True, calculate_bleu = True):\n",
    "\n",
    "    \n",
    "    #Evaluation Mode\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    \n",
    "    references = list()\n",
    "    hypotheses = list() \n",
    "    \n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        count    = 0\n",
    "        loss_avg = 0\n",
    "        bleu1_avg = 0\n",
    "        bleu4_avg = 0\n",
    "        \n",
    "        for i, (inputs, caps, caplens, allcaps) in enumerate(val_loader):\n",
    "\n",
    "            # Move to device, if available\n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)\n",
    "                caps = caps.to(device)\n",
    "\n",
    "                        \n",
    "            enc_out = encoder(inputs)\n",
    "            actual_lengths = allcaps\n",
    "            \n",
    "            \n",
    "            \n",
    "            temperature = 1\n",
    "            test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature)\n",
    "\n",
    "            test_pred_sample = test_pred[0].cpu().numpy()          \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            #Build a list of the predicted sentences\n",
    "            # Convert word_ids to words\n",
    "            sampled_caption = []\n",
    "\n",
    "            for word_id in test_pred_sample:\n",
    "                word = vocab.idx2word[word_id]\n",
    "                sampled_caption.append(word)\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "            sentence = ' '.join(sampled_caption)\n",
    "            hypotheses.append(sampled_caption)                \n",
    "#             if i % 100 ==0:\n",
    "#                 print ('generated sentence: ',sentence)            \n",
    "#                 print(type(sampled_caption))        \n",
    "#                 print(sampled_caption)        \n",
    "#                 print('len(generated_sentence): ',len(sampled_caption))\n",
    "\n",
    "                \n",
    "            #targets = pack_padded_sequence(labels, lengths, batch_first=True)[0]\n",
    "        \n",
    "            outputs = decoder(caps, enc_out, actual_lengths)\n",
    "            \n",
    "            new_outputs = torch.zeros(inputs.shape[0], maxSeqLen, vocab.idx)\n",
    "            for dim in range(maxSeqLen):\n",
    "                for b in range(batch_size):\n",
    "                    new_outputs[b, dim, 0] = 1.0\n",
    "            new_outputs[:, :(outputs.shape[1]), :] = outputs\n",
    "            new_outputs = new_outputs.permute(0, 2, 1).to(device)\n",
    "            \n",
    "            #del inputs\n",
    "\n",
    "            \n",
    "            loss = criterion(new_outputs, Variable(caps.long()))\n",
    "            loss_avg += loss\n",
    "            count+=1\n",
    "            \n",
    "            #del outputs            \n",
    "            \n",
    "            #print('VAL: loss: ', loss)\n",
    "\n",
    "            \n",
    "            \n",
    "            caps_array = caps.cpu().numpy()  \n",
    "            # Convert word_ids to words\n",
    "            reference_caption = []\n",
    "            sampled_caption = []\n",
    "            \n",
    "            for word_id in caps_array[0]:\n",
    "                word = vocab.idx2word[word_id]\n",
    "                reference_caption.append(word)\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "            ref_sentence = ' '.join(reference_caption)\n",
    "            #print('ref_sentence: ', ref_sentence)\n",
    "            #print('len(ref_sentence): ',len(reference_caption))\n",
    "            references.append(reference_caption)   \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "            #print('len(references)', len(references))\n",
    "            #print('len(hypotheses)', len(hypotheses))\n",
    "            #print('references: ', references)\n",
    "            #print('hypotheses: ', hypotheses)\n",
    "        \n",
    "\n",
    "            # Calculate BLEU-4 scores\n",
    "            if calculate_bleu:\n",
    "                bleu4 = corpus_bleu(references, hypotheses)                \n",
    "                bleu1 = corpus_bleu(references, hypotheses,weights=(1.0, 0, 0, 0))\n",
    "                #print('bleu4: ', bleu4)        \n",
    "                #print('bleu1: ', bleu1)  \n",
    "                bleu4_avg+=bleu4\n",
    "                bleu1_avg+=bleu1\n",
    "            \n",
    "            del caps\n",
    "            del outputs            \n",
    "            \n",
    "            \n",
    "            #if i % 100 == 0:\n",
    "            #    break\n",
    "                \n",
    "                \n",
    "        if calculate_bleu:\n",
    "            loss_avg  = loss_avg/count\n",
    "            bleu4_avg = bleu4_avg/count\n",
    "            bleu1_avg = bleu1_avg/count \n",
    "            #print('VAL: loss_avg: ', loss_avg)\n",
    "            #print('VAL: bleu4_avg: ', bleu4_avg)\n",
    "            #print('VAL: bleu1_avg: ', bleu1_avg)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    return loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEncoderDecoder(encoder, decoder, criterion, epochs,\n",
    "                        train_loader,val_loader, test_loader,\n",
    "                        name, batch_size, maxSeqLen, vocab):\n",
    "    \n",
    "    #Create non-existing logfiles\n",
    "    logname = './logs/' + name + '.log'\n",
    "    i = 0\n",
    "    if os.path.exists(logname) == True:\n",
    "        \n",
    "        logname = './logs/' + name + str(i) + '.log'\n",
    "        while os.path.exists(logname):\n",
    "            i+=1\n",
    "            logname = './logs/' + name + str(i) + '.log'\n",
    "\n",
    "    print('Loading results to logfile: ' + logname)\n",
    "    with open(logname, \"w\") as file:\n",
    "        file.write(\"Log file DATA: Validation Loss and Accuracy\\n\") \n",
    "    \n",
    "    logname_summary = './logs/' + name + '_summary' + str(i) + '.log'    \n",
    "    print('Loading Summary to : ' + logname_summary) \n",
    "    \n",
    "    parameters = list(encoder.fc.parameters())\n",
    "    parameters.extend(list(decoder.parameters()))\n",
    "    optimizer = optim.Adam(parameters, lr=5e-5)\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "#         encoder = torch.nn.DataParallel(encoder)\n",
    "#         decoder = torch.nn.DataParallel(decoder)\n",
    "        \n",
    "        encoder.to(device)\n",
    "        decoder.to(device)\n",
    "        \n",
    "        \n",
    "    \n",
    "    val_loss_set = []\n",
    "    val_acc_set = []\n",
    "    val_iou_set = []\n",
    "    \n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    # Early Stop criteria\n",
    "    minLoss = 1e6\n",
    "    minLossIdx = 0\n",
    "    earliestStopEpoch = 10\n",
    "    earlyStopDelta = 5\n",
    "    for epoch in range(epochs):\n",
    "        ts = time.time()\n",
    "\n",
    "        #import pdb; pdb.set_trace()                     \n",
    "        for iter, (inputs, labels, lengths, actual_lengths) in tqdm(enumerate(train_loader)):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)# Move your inputs onto the gpu\n",
    "                labels = labels.to(device) # Move your labels onto the gpu\n",
    "            \n",
    "                \n",
    "            enc_out = encoder(inputs)\n",
    "            #print('enc out shape: ', enc_out.size())\n",
    "        \n",
    "            temperature = 1\n",
    "            \n",
    "            \n",
    "            targets = pack_padded_sequence(labels, lengths, batch_first=True)[0]\n",
    "            \n",
    "            \n",
    "            outputs = decoder(labels, enc_out, actual_lengths) #calls forward\n",
    "            #print('outputs shape: ', outputs.size())\n",
    "            new_outputs = torch.zeros(inputs.shape[0], maxSeqLen, vocab.idx)\n",
    "            new_outputs[:inputs.shape[0],:maxSeqLen, 0] = torch.ones((inputs.shape[0], maxSeqLen))\n",
    "#             for dim in range(maxSeqLen):\n",
    "#                 for b in range(inputs.shape[0]):\n",
    "#                     new_outputs[b, dim, 0] = 1.0\n",
    "            new_outputs[:, :(outputs.shape[1]), :] = outputs\n",
    "            new_outputs = new_outputs.permute(0, 2, 1).to(device)\n",
    "\n",
    "            \n",
    "            loss = criterion(new_outputs, Variable(labels.long()))\n",
    "            del labels\n",
    "            del outputs\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss))\n",
    "            \n",
    "\n",
    "\n",
    "        print(\"epoch{}, iter{}, loss: {}, epoch duration: {}\".format(epoch, iter, loss, time.time() - ts))\n",
    "        test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature).cpu()\n",
    "\n",
    "        for b in range(inputs.shape[0]):\n",
    "            caption = (\" \").join([vocab.idx2word[x.item()] for x in test_pred[b]])\n",
    "            img = tf.ToPILImage()(inputs[b,:,:,:].cpu())\n",
    "            plt.imshow(img)\n",
    "                    \n",
    "            plt.show()\n",
    "            print(\"Caption: \" + caption)\n",
    "        # calculate val loss each epoch\n",
    "        val_loss  = validate(val_loader, encoder, decoder, criterion,maxSeqLen, vocab.idx, use_gpu)\n",
    "        val_loss_set.append(val_loss)\n",
    "\n",
    "        \n",
    "#         print(\"epoch {}, time {}, train loss {}, val loss {}, val acc {}, val iou {}\".format(epoch, time.time() - ts,\n",
    "#                                                                                                loss, val_loss,\n",
    "#                                                                                                val_acc,\n",
    "#                                                                                                val_iou))        \n",
    "        training_loss.append(loss)\n",
    "        \n",
    "        with open(logname, \"a\") as file:\n",
    "            file.write(\"writing!\\n\")\n",
    "            file.write(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "            file.write(\"\\n training Loss:   \" + str(loss.item()))\n",
    "#             file.write(\"\\n Validation Loss: \" + str(val_loss_set[-1]))\n",
    "#             file.write(\"\\n Validation acc:  \" + str(val_acc_set[-1]))\n",
    "#             file.write(\"\\n Validation iou:  \" + str(val_iou_set[-1]) + \"\\n \")                                             \n",
    "                                                                                                \n",
    "                                                                                                \n",
    "        \n",
    "        # Early stopping\n",
    "#         if val_loss < minLoss:\n",
    "#             # Store new best\n",
    "#             torch.save(model, name)\n",
    "#             minLoss = val_loss#.item()\n",
    "#             minLossIdx = epoch\n",
    "            \n",
    "        # If passed min threshold, and no new min has been reached for delta epochs\n",
    "#         elif epoch > earliestStopEpoch and (epoch - minLossIdx) > earlyStopDelta:\n",
    "#             print(\"Stopping early at {}\".format(minLossIdx))\n",
    "#             break\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    with open(logname_summary, \"a\") as file:\n",
    "            file.write(\"Summary!\\n\")\n",
    "            file.write(\"Stopped early at {}\".format(minLossIdx))\n",
    "            file.write(\"\\n training Loss:   \" + str(training_loss))        \n",
    "            file.write(\"\\n Validation Loss: \" + str(val_loss_set))\n",
    "            file.write(\"\\n Validation acc:  \" + str(val_acc_set))\n",
    "            file.write(\"\\n Validation iou:  \" + str(val_iou_set) + \"\\n \")\n",
    "            \n",
    "        \n",
    "    #return val_loss_set, val_acc_set, val_iou_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading results to logfile: ./logs/LSTM59.log\n",
      "Loading Summary to : ./logs/LSTM_summary59.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter0, loss: -8.612080273451284e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:34,  2.83it/s]"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    with open('TrainImageIds.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        trainIds = list(reader)[0]\n",
    "        \n",
    "    with open('TestImageIds.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        testIds = list(reader)[0]\n",
    "    \n",
    "    trainIds = [int(i) for i in trainIds]\n",
    "    testIds = [int(i) for i in testIds]\n",
    "    \n",
    "    # Will shuffle the trainIds incase of ordering in csv\n",
    "    random.shuffle(trainIds)\n",
    "    splitIdx = int(len(trainIds)/5)\n",
    "    \n",
    "    # Selecting 1/5 of training set as validation\n",
    "    valIds = trainIds[:splitIdx]\n",
    "    trainIds = trainIds[splitIdx:]\n",
    "    #print(trainIds)\n",
    "    \n",
    "    \n",
    "    trainValRoot = \"./data/images/train/\"\n",
    "    testRoot = \"./data/images/test/\"\n",
    "    \n",
    "    trainValJson = \"./data/annotations/captions_train2014.json\"\n",
    "    testJson = \"./data/annotations/captions_val2014.json\"\n",
    "    \n",
    "    \n",
    "    with open('./data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    img_side_length = 512\n",
    "    transform = tf.Compose([\n",
    "        tf.Resize(img_side_length),\n",
    "        #tf.RandomCrop(img_side_length),\n",
    "        tf.CenterCrop(img_side_length),\n",
    "        tf.ToTensor(),\n",
    "    ])\n",
    "    batch_size = 20\n",
    "    shuffle = True\n",
    "    num_workers = 4\n",
    "    \n",
    "    \n",
    "    trainDl = get_loader(trainValRoot, trainValJson, trainIds, vocab, \n",
    "                         transform=transform, batch_size=batch_size, \n",
    "                         shuffle=False, num_workers=num_workers)\n",
    "    valDl = get_loader(trainValRoot, trainValJson, valIds, vocab, \n",
    "                         transform=transform, batch_size=batch_size, \n",
    "                         shuffle=shuffle, num_workers=num_workers)\n",
    "    testDl = get_loader(testRoot, testJson, testIds, vocab, \n",
    "                        transform=transform, batch_size=batch_size, \n",
    "                        shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    encoded_feature_dim = 56\n",
    "    hidden_dim = 50\n",
    "    \n",
    "    encoder = Encoder(encoded_feature_dim)\n",
    "    # Turn off all gradients in encoder\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Turn on gradient of final hidden layer for fine tuning\n",
    "    for param in encoder.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    decoder = Decoder(encoded_feature_dim, hidden_dim, vocab.idx)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    epochs = 100\n",
    "    trainEncoderDecoder(encoder, decoder, criterion, epochs,\n",
    "                        trainDl, valDl, testDl, \"LSTM\",\n",
    "                        batch_size, encoded_feature_dim, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
