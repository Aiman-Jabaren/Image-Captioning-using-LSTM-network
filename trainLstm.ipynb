{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder_new import *\n",
    "from encoder import *\n",
    "from data_loader import *\n",
    "import pickle\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import torchvision.transforms as tf\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate(val_loader, encoder, decoder, criterion, maxSeqLen, vocablen, use_gpu = True):\n",
    "\n",
    "    #Evaluation Mode\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        count    = 0\n",
    "        loss_avg = 0\n",
    "        \n",
    "        \n",
    "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
    "\n",
    "            # Move to device, if available\n",
    "            if use_gpu:\n",
    "                imgs = imgs.to(device)\n",
    "                caps = caps.to(device)\n",
    "            \n",
    "            if (i % 1000) == 0:\n",
    "                print('i: ',i)\n",
    "                print('imgs: ',imgs)\n",
    "                print('caps: ',caps)\n",
    "                print('val_loader: ', val_loader)\n",
    "                print('caplens: ',caplens)\n",
    "            \n",
    "            enc_out = encoder(imgs)\n",
    "            actual_lengths = allcaps\n",
    "            \n",
    "            \n",
    "            \n",
    "            temperature = 1\n",
    "            test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature)\n",
    "\n",
    "            test_pred_sample = test_pred[0].cpu().numpy()          \n",
    "\n",
    "            \n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                # Convert word_ids to words\n",
    "                sampled_caption = []\n",
    "                for word_id in test_pred_sample:\n",
    "                    word = vocab.idx2word[word_id]\n",
    "                    sampled_caption.append(word)\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                sentence = ' '.join(sampled_caption)\n",
    "                print ('sentence: ',sentence)            \n",
    "                        \n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                flag = False\n",
    "                for wi in range(maxSeqLen):\n",
    "                    if test_pred[b, wi] == 2:\n",
    "                        flag = True\n",
    "                    if flag:\n",
    "                        test_pred[b, wi] = 0\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print('VAL: test pred: ', test_pred)\n",
    "                print('VAL: test pred shape: ', test_pred.size())\n",
    "        \n",
    "            outputs = decoder(caps, enc_out, actual_lengths)\n",
    "            if i % 1000 == 0:\n",
    "                print('VAL: outputs shape: ', outputs.size())\n",
    "            new_outputs = torch.zeros(batch_size, maxSeqLen, vocablen)\n",
    "            for dim in range(maxSeqLen):\n",
    "                for b in range(batch_size):\n",
    "                    new_outputs[b, dim, 0] = 1.0\n",
    "            new_outputs[:, :(outputs.shape[1]), :] = outputs\n",
    "            new_outputs = new_outputs.permute(0, 2, 1).to(device)\n",
    "            #del inputs\n",
    "            loss = criterion(new_outputs, Variable(caps.long()))\n",
    "            loss_avg += loss\n",
    "            count+=1\n",
    "            \n",
    "            del caps\n",
    "            del outputs            \n",
    "            \n",
    "            print('VAL: loss: ', loss)\n",
    "#            if i % 100 == 0:\n",
    "#                break\n",
    "        \n",
    "        loss_avg = loss_avg/count\n",
    "        print('VAL: loss_avg: ', loss_avg)\n",
    "            \n",
    "    return loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEncoderDecoder(encoder, decoder, criterion, epochs,\n",
    "                        train_loader,val_loader, test_loader, name, batch_size, maxSeqLen, vocablen):\n",
    "    \n",
    "    #Create non-existing logfiles\n",
    "    logname = './logs/' + name + '.log'\n",
    "    i = 0\n",
    "    if os.path.exists(logname) == True:\n",
    "        \n",
    "        logname = './logs/' + name + str(i) + '.log'\n",
    "        while os.path.exists(logname):\n",
    "            i+=1\n",
    "            logname = './logs/' + name + str(i) + '.log'\n",
    "\n",
    "    print('Loading results to logfile: ' + logname)\n",
    "    with open(logname, \"w\") as file:\n",
    "        file.write(\"Log file DATA: Validation Loss and Accuracy\\n\") \n",
    "    \n",
    "    logname_summary = './logs/' + name + '_summary' + str(i) + '.log'    \n",
    "    print('Loading Summary to : ' + logname_summary) \n",
    "    \n",
    "    parameters = list(encoder.parameters())\n",
    "    parameters.extend(list(decoder.parameters()))\n",
    "    optimizer = optim.Adam(parameters, lr=5e-3)\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "#         encoder = torch.nn.DataParallel(encoder)\n",
    "#         decoder = torch.nn.DataParallel(decoder)\n",
    "        \n",
    "        encoder.to(device)\n",
    "        decoder.to(device)\n",
    "        \n",
    "        \n",
    "    \n",
    "    val_loss_set = []\n",
    "    val_acc_set = []\n",
    "    val_iou_set = []\n",
    "    \n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    # Early Stop criteria\n",
    "    minLoss = 1e6\n",
    "    minLossIdx = 0\n",
    "    earliestStopEpoch = 10\n",
    "    earlyStopDelta = 5\n",
    "    for epoch in range(epochs):\n",
    "        ts = time.time()\n",
    "\n",
    "        #import pdb; pdb.set_trace()                     \n",
    "        for iter, (inputs, labels, lengths, actual_lengths) in tqdm(enumerate(train_loader)):\n",
    "            print('inp shape: ', inputs.shape)\n",
    "#             print(\"Inputs:\")\n",
    "#             print(inputs)\n",
    "            \n",
    "            print(\"Labels\")\n",
    "            print(labels)\n",
    "            print('labels shape: ', labels.shape)\n",
    "            print(\"lengths: \", lengths)\n",
    "            print(\"lengths shape: \", len(lengths))\n",
    "            print(\"actual_lengths: \", actual_lengths)\n",
    "            print(\"actual_lengths shape: \", len(actual_lengths))\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)# Move your inputs onto the gpu\n",
    "                labels = labels.to(device) # Move your labels onto the gpu\n",
    "            \n",
    "                \n",
    "#             outputs = model(inputs)\n",
    "            enc_out = encoder(inputs)\n",
    "            print('enc out shape: ', enc_out.size())\n",
    "        \n",
    "            temperature = 1\n",
    "            test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature)\n",
    "            for b in range(batch_size):\n",
    "                flag = False\n",
    "                for wi in range(maxSeqLen):\n",
    "                    if test_pred[b, wi] == 2:\n",
    "                        flag = True\n",
    "                    if flag:\n",
    "                        test_pred[b, wi] = 0\n",
    "            print('test pred: ', test_pred)\n",
    "            print('test pred shape: ', test_pred.size())\n",
    "        \n",
    "            outputs = decoder(labels, enc_out, actual_lengths) #calls forward\n",
    "            print('outputs shape: ', outputs.size())\n",
    "            new_outputs = torch.zeros(batch_size, maxSeqLen, vocablen)\n",
    "            for dim in range(maxSeqLen):\n",
    "                for b in range(batch_size):\n",
    "                    new_outputs[b, dim, 0] = 1.0\n",
    "            new_outputs[:, :(outputs.shape[1]), :] = outputs\n",
    "            new_outputs = new_outputs.permute(0, 2, 1).to(device)\n",
    "            #del inputs\n",
    "            loss = criterion(new_outputs, Variable(labels.long()))\n",
    "            del labels\n",
    "            del outputs\n",
    "\n",
    "            loss.backward()\n",
    "            loss = loss#.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 1 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss))\n",
    "            \n",
    "#            if iter >=100:\n",
    "#                break\n",
    "\n",
    "        \n",
    "        # calculate val loss each epoch\n",
    "        val_loss  = validate(val_loader, encoder, decoder, criterion,maxSeqLen, vocablen, use_gpu)\n",
    "        val_loss_set.append(val_loss)\n",
    "\n",
    "        \n",
    "#         print(\"epoch {}, time {}, train loss {}, val loss {}, val acc {}, val iou {}\".format(epoch, time.time() - ts,\n",
    "#                                                                                                loss, val_loss,\n",
    "#                                                                                                val_acc,\n",
    "#                                                                                                val_iou))        \n",
    "        training_loss.append(loss)\n",
    "        \n",
    "        with open(logname, \"a\") as file:\n",
    "            file.write(\"writing!\\n\")\n",
    "            file.write(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "            file.write(\"\\n training Loss:   \" + str(loss.item()))\n",
    "#             file.write(\"\\n Validation Loss: \" + str(val_loss_set[-1]))\n",
    "#             file.write(\"\\n Validation acc:  \" + str(val_acc_set[-1]))\n",
    "#             file.write(\"\\n Validation iou:  \" + str(val_iou_set[-1]) + \"\\n \")                                             \n",
    "                                                                                                \n",
    "                                                                                                \n",
    "        \n",
    "        # Early stopping\n",
    "#         if val_loss < minLoss:\n",
    "#             # Store new best\n",
    "#             torch.save(model, name)\n",
    "#             minLoss = val_loss#.item()\n",
    "#             minLossIdx = epoch\n",
    "            \n",
    "        # If passed min threshold, and no new min has been reached for delta epochs\n",
    "#         elif epoch > earliestStopEpoch and (epoch - minLossIdx) > earlyStopDelta:\n",
    "#             print(\"Stopping early at {}\".format(minLossIdx))\n",
    "#             break\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    with open(logname_summary, \"a\") as file:\n",
    "            file.write(\"Summary!\\n\")\n",
    "            file.write(\"Stopped early at {}\".format(minLossIdx))\n",
    "            file.write(\"\\n training Loss:   \" + str(training_loss))        \n",
    "            file.write(\"\\n Validation Loss: \" + str(val_loss_set))\n",
    "            file.write(\"\\n Validation acc:  \" + str(val_acc_set))\n",
    "            file.write(\"\\n Validation iou:  \" + str(val_iou_set) + \"\\n \")\n",
    "            \n",
    "        \n",
    "    #return val_loss_set, val_acc_set, val_iou_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    with open('TrainImageIds.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        trainIds = list(reader)[0]\n",
    "        \n",
    "#     with open('TestImageIds.csv', 'r') as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         testIds = list(reader)[0]\n",
    "    \n",
    "    trainIds = [int(i) for i in trainIds]\n",
    "    #testIds = [int(i) for i in testIds[0]]\n",
    "    \n",
    "    # Will shuffle the trainIds incase of ordering in csv\n",
    "    #random.shuffle(trainIds)\n",
    "    #splitIdx = int(len(trainIds)/5)\n",
    "    \n",
    "    # Selecting 1/5 of training set as validation\n",
    "    #valIds = trainIds[:splitIdx]\n",
    "    #trainIds = trainIds[splitIdx:]\n",
    "    #print(trainIds)\n",
    "    \n",
    "    \n",
    "    trainValRoot = \"./data/images/train/\"\n",
    "    #testRoot = \"./data/images/test/\"\n",
    "    \n",
    "    trainValJson = \"./data/annotations/captions_train2014.json\"\n",
    "    #testJson = \"./data/annotations/captions_val2014.json\"\n",
    "    \n",
    "    \n",
    "    with open('./data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    img_side_length = 700\n",
    "    transform = tf.Compose([\n",
    "        tf.Resize(img_side_length),\n",
    "        #tf.RandomCrop(img_side_length),\n",
    "        tf.CenterCrop(img_side_length),\n",
    "        tf.ToTensor(),\n",
    "    ])\n",
    "    batch_size = 4\n",
    "    shuffle = True\n",
    "    num_workers = 1\n",
    "    \n",
    "    \n",
    "    trainDl = get_loader(trainValRoot, trainValJson, trainIds, vocab, \n",
    "                         transform=transform, batch_size=batch_size, \n",
    "                         shuffle=False, num_workers=1)\n",
    "#     valDl = get_loader(trainValRoot, trainValJson, valIds, vocab, \n",
    "#                          transform=None, batch_size=batch_size, \n",
    "#                          shuffle=shuffle, num_workers=1)\n",
    "#    testDl = get_loader(testRoot, testJson, testIds, vocab, \n",
    "#                         transform=None, batch_size=batch_size, \n",
    "#                         shuffle=shuffle, num_workers=1)\n",
    "    \n",
    "    encoded_feature_dim = 56\n",
    "    hidden_dim = 50\n",
    "    \n",
    "    encoder = Encoder(encoded_feature_dim)\n",
    "    decoder = Decoder(encoded_feature_dim, hidden_dim, vocab.idx)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    epochs = 100\n",
    "    trainEncoderDecoder(encoder, decoder, criterion, epochs, \n",
    "                        trainDl, trainDl, trainDl, \"LSTM\", batch_size, encoded_feature_dim, vocab.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
