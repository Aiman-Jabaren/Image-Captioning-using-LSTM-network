{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /datasets/home/64/364/rhadden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from decoder import *\n",
    "from encoder import *\n",
    "from data_loader import *\n",
    "import pickle\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEncoderDecoder(encoder, decoder, criterion, epochs,\n",
    "                        train_loader,val_loader, test_loader, name):\n",
    "    \n",
    "    #Create non-existing logfiles\n",
    "    logname = './logs/' + name + '.log'\n",
    "    i = 0\n",
    "    if os.path.exists(logname) == True:\n",
    "        \n",
    "        logname = './logs/' + name + str(i) + '.log'\n",
    "        while os.path.exists(logname):\n",
    "            i+=1\n",
    "            logname = './logs/' + name + str(i) + '.log'\n",
    "\n",
    "    print('Loading results to logfile: ' + logname)\n",
    "    with open(logname, \"a\") as file:\n",
    "        file.write(\"Log file DATA: Validation Loss and Accuracy\\n\") \n",
    "    \n",
    "    logname_summary = './logs/' + name + '_summary' + str(i) + '.log'    \n",
    "    print('Loading Summary to : ' + logname_summary) \n",
    "    \n",
    "    parameters = list(encoder.parameters())\n",
    "    parameters.extend(list(decoder.parameters()))\n",
    "    optimizer = optim.Adam(parameters, lr=5e-3)\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        encoder = torch.nn.DataParallel(encoder)\n",
    "        decoder = torch.nn.DataParallel(decoder)\n",
    "        \n",
    "        encoder.to(device)\n",
    "        decoder.to(device)\n",
    "        \n",
    "        \n",
    "    \n",
    "    val_loss_set = []\n",
    "    val_acc_set = []\n",
    "    val_iou_set = []\n",
    "    \n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    # Early Stop criteria\n",
    "    minLoss = 1e6\n",
    "    minLossIdx = 0\n",
    "    earliestStopEpoch = 10\n",
    "    earlyStopDelta = 5\n",
    "    for epoch in range(epochs):\n",
    "        ts = time.time()\n",
    "\n",
    "        #import pdb; pdb.set_trace()                     \n",
    "        for iter, (inputs, labels, lengths) in tqdm(enumerate(train_loader)):\n",
    "            print(\"Inputs:\")\n",
    "            print(inputs)\n",
    "            \n",
    "            print(\"Labels\")\n",
    "            print(labels)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)# Move your inputs onto the gpu\n",
    "                labels = labels.to(device) # Move your labels onto the gpu\n",
    "            \n",
    "                \n",
    "            #outputs = model(inputs)\n",
    "            #del inputs\n",
    "            loss = criterion(outputs, Variable(labels.long()))\n",
    "            del labels\n",
    "            del outputs\n",
    "\n",
    "            loss.backward()\n",
    "            loss = loss#.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 10 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss))\n",
    "\n",
    "        \n",
    "        # calculate val loss each epoch\n",
    "#         val_loss, val_acc, val_iou = val(model, val_loader, criterion, use_gpu)\n",
    "#         val_loss_set.append(val_loss)\n",
    "#         val_acc_set.append(val_acc)\n",
    "#         val_iou_set.append(val_iou)\n",
    "        \n",
    "#         print(\"epoch {}, time {}, train loss {}, val loss {}, val acc {}, val iou {}\".format(epoch, time.time() - ts,\n",
    "#                                                                                                loss, val_loss,\n",
    "#                                                                                                val_acc,\n",
    "#                                                                                                val_iou))        \n",
    "        training_loss.append(loss)\n",
    "        \n",
    "        with open(logname, \"a\") as file:\n",
    "            file.write(\"writing!\\n\")\n",
    "            file.write(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "            file.write(\"\\n training Loss:   \" + str(loss.item()))\n",
    "#             file.write(\"\\n Validation Loss: \" + str(val_loss_set[-1]))\n",
    "#             file.write(\"\\n Validation acc:  \" + str(val_acc_set[-1]))\n",
    "#             file.write(\"\\n Validation iou:  \" + str(val_iou_set[-1]) + \"\\n \")                                             \n",
    "                                                                                                \n",
    "                                                                                                \n",
    "        \n",
    "        # Early stopping\n",
    "#         if val_loss < minLoss:\n",
    "#             # Store new best\n",
    "#             torch.save(model, name)\n",
    "#             minLoss = val_loss#.item()\n",
    "#             minLossIdx = epoch\n",
    "            \n",
    "        # If passed min threshold, and no new min has been reached for delta epochs\n",
    "#         elif epoch > earliestStopEpoch and (epoch - minLossIdx) > earlyStopDelta:\n",
    "#             print(\"Stopping early at {}\".format(minLossIdx))\n",
    "#             break\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    with open(logname_summary, \"a\") as file:\n",
    "            file.write(\"Summary!\\n\")\n",
    "            file.write(\"Stopped early at {}\".format(minLossIdx))\n",
    "            file.write(\"\\n training Loss:   \" + str(training_loss))        \n",
    "            file.write(\"\\n Validation Loss: \" + str(val_loss_set))\n",
    "            file.write(\"\\n Validation acc:  \" + str(val_acc_set))\n",
    "            file.write(\"\\n Validation iou:  \" + str(val_iou_set) + \"\\n \")\n",
    "            \n",
    "        \n",
    "    #return val_loss_set, val_acc_set, val_iou_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.84s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading results to logfile: ./logs/LSTM23.log\n",
      "Loading Summary to : ./logs/LSTM_summary23.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "ann id: 661353\n",
      "Img id: 509365\n",
      "[(tensor([[[0.5412, 0.5412, 0.5373,  ..., 0.5412, 0.5333, 0.5373],\n",
      "         [0.5451, 0.5451, 0.5412,  ..., 0.5412, 0.5373, 0.5412],\n",
      "         [0.5451, 0.5451, 0.5451,  ..., 0.5412, 0.5412, 0.5451],\n",
      "         ...,\n",
      "         [0.4157, 0.4078, 0.4510,  ..., 0.4667, 0.4431, 0.4471],\n",
      "         [0.4471, 0.4392, 0.4510,  ..., 0.4196, 0.4510, 0.4902],\n",
      "         [0.3922, 0.4549, 0.4431,  ..., 0.4314, 0.4353, 0.4941]],\n",
      "\n",
      "        [[0.6745, 0.6745, 0.6784,  ..., 0.6745, 0.6667, 0.6706],\n",
      "         [0.6784, 0.6784, 0.6824,  ..., 0.6745, 0.6706, 0.6745],\n",
      "         [0.6784, 0.6784, 0.6784,  ..., 0.6745, 0.6745, 0.6784],\n",
      "         ...,\n",
      "         [0.4431, 0.4588, 0.5059,  ..., 0.4941, 0.4588, 0.4471],\n",
      "         [0.4588, 0.4706, 0.4863,  ..., 0.4314, 0.4588, 0.4863],\n",
      "         [0.4235, 0.5020, 0.4824,  ..., 0.4353, 0.4314, 0.4863]],\n",
      "\n",
      "        [[0.8196, 0.8196, 0.8196,  ..., 0.8196, 0.8118, 0.8157],\n",
      "         [0.8235, 0.8235, 0.8235,  ..., 0.8196, 0.8157, 0.8196],\n",
      "         [0.8235, 0.8235, 0.8235,  ..., 0.8196, 0.8196, 0.8235],\n",
      "         ...,\n",
      "         [0.2431, 0.2510, 0.2980,  ..., 0.2941, 0.2627, 0.2510],\n",
      "         [0.2824, 0.2784, 0.2784,  ..., 0.2549, 0.2745, 0.2980],\n",
      "         [0.2706, 0.3216, 0.2863,  ..., 0.2784, 0.2588, 0.2980]]]), tensor([  1.,   4.,  81.,  14., 723., 225.,  40.,   4., 133.,  80.,  33., 279.,\n",
      "         19.,   2.]))]\n",
      "Index: 1\n",
      "ann id: 663117\n",
      "Img id: 509365\n",
      "[(tensor([[[0.5412, 0.5412, 0.5373,  ..., 0.5412, 0.5333, 0.5373],\n",
      "         [0.5451, 0.5451, 0.5412,  ..., 0.5412, 0.5373, 0.5412],\n",
      "         [0.5451, 0.5451, 0.5451,  ..., 0.5412, 0.5412, 0.5451],\n",
      "         ...,\n",
      "         [0.4157, 0.4078, 0.4510,  ..., 0.4667, 0.4431, 0.4471],\n",
      "         [0.4471, 0.4392, 0.4510,  ..., 0.4196, 0.4510, 0.4902],\n",
      "         [0.3922, 0.4549, 0.4431,  ..., 0.4314, 0.4353, 0.4941]],\n",
      "\n",
      "        [[0.6745, 0.6745, 0.6784,  ..., 0.6745, 0.6667, 0.6706],\n",
      "         [0.6784, 0.6784, 0.6824,  ..., 0.6745, 0.6706, 0.6745],\n",
      "         [0.6784, 0.6784, 0.6784,  ..., 0.6745, 0.6745, 0.6784],\n",
      "         ...,\n",
      "         [0.4431, 0.4588, 0.5059,  ..., 0.4941, 0.4588, 0.4471],\n",
      "         [0.4588, 0.4706, 0.4863,  ..., 0.4314, 0.4588, 0.4863],\n",
      "         [0.4235, 0.5020, 0.4824,  ..., 0.4353, 0.4314, 0.4863]],\n",
      "\n",
      "        [[0.8196, 0.8196, 0.8196,  ..., 0.8196, 0.8118, 0.8157],\n",
      "         [0.8235, 0.8235, 0.8235,  ..., 0.8196, 0.8157, 0.8196],\n",
      "         [0.8235, 0.8235, 0.8235,  ..., 0.8196, 0.8196, 0.8235],\n",
      "         ...,\n",
      "         [0.2431, 0.2510, 0.2980,  ..., 0.2941, 0.2627, 0.2510],\n",
      "         [0.2824, 0.2784, 0.2784,  ..., 0.2549, 0.2745, 0.2980],\n",
      "         [0.2706, 0.3216, 0.2863,  ..., 0.2784, 0.2588, 0.2980]]]), tensor([  1.,   4.,  81.,  14., 455., 225.,  40.,   4., 133.,  80.,  33., 279.,\n",
      "          2.]))]\n",
      "Inputs:\n",
      "tensor([[[[0.5412, 0.5412, 0.5373,  ..., 0.5412, 0.5333, 0.5373],\n",
      "          [0.5451, 0.5451, 0.5412,  ..., 0.5412, 0.5373, 0.5412],\n",
      "          [0.5451, 0.5451, 0.5451,  ..., 0.5412, 0.5412, 0.5451],\n",
      "          ...,\n",
      "          [0.4157, 0.4078, 0.4510,  ..., 0.4667, 0.4431, 0.4471],\n",
      "          [0.4471, 0.4392, 0.4510,  ..., 0.4196, 0.4510, 0.4902],\n",
      "          [0.3922, 0.4549, 0.4431,  ..., 0.4314, 0.4353, 0.4941]],\n",
      "\n",
      "         [[0.6745, 0.6745, 0.6784,  ..., 0.6745, 0.6667, 0.6706],\n",
      "          [0.6784, 0.6784, 0.6824,  ..., 0.6745, 0.6706, 0.6745],\n",
      "          [0.6784, 0.6784, 0.6784,  ..., 0.6745, 0.6745, 0.6784],\n",
      "          ...,\n",
      "          [0.4431, 0.4588, 0.5059,  ..., 0.4941, 0.4588, 0.4471],\n",
      "          [0.4588, 0.4706, 0.4863,  ..., 0.4314, 0.4588, 0.4863],\n",
      "          [0.4235, 0.5020, 0.4824,  ..., 0.4353, 0.4314, 0.4863]],\n",
      "\n",
      "         [[0.8196, 0.8196, 0.8196,  ..., 0.8196, 0.8118, 0.8157],\n",
      "          [0.8235, 0.8235, 0.8235,  ..., 0.8196, 0.8157, 0.8196],\n",
      "          [0.8235, 0.8235, 0.8235,  ..., 0.8196, 0.8196, 0.8235],\n",
      "          ...,\n",
      "          [0.2431, 0.2510, 0.2980,  ..., 0.2941, 0.2627, 0.2510],\n",
      "          [0.2824, 0.2784, 0.2784,  ..., 0.2549, 0.2745, 0.2980],\n",
      "          [0.2706, 0.3216, 0.2863,  ..., 0.2784, 0.2588, 0.2980]]]])\n",
      "Labels\n",
      "tensor([[  1,   4,  81,  14, 723, 225,  40,   4, 133,  80,  33, 279,  19,   2]])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'outputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3b18e0a04994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     trainEncoderDecoder(encoder, decoder, criterion, epochs, \n\u001b[0;32m---> 59\u001b[0;31m                         trainDl, trainDl, trainDl, \"LSTM\")\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4f24804ff72e>\u001b[0m in \u001b[0;36mtrainEncoderDecoder\u001b[0;34m(encoder, decoder, criterion, epochs, train_loader, val_loader, test_loader, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#outputs = model(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m#del inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'outputs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    with open('TrainImageIds.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        trainIds = list(reader)[0]\n",
    "        \n",
    "#     with open('TestImageIds.csv', 'r') as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         testIds = list(reader)[0]\n",
    "    \n",
    "    trainIds = [int(i) for i in trainIds]\n",
    "    #testIds = [int(i) for i in testIds[0]]\n",
    "    \n",
    "    # Will shuffle the trainIds incase of ordering in csv\n",
    "    #random.shuffle(trainIds)\n",
    "    #splitIdx = int(len(trainIds)/5)\n",
    "    \n",
    "    # Selecting 1/5 of training set as validation\n",
    "    #valIds = trainIds[:splitIdx]\n",
    "    #trainIds = trainIds[splitIdx:]\n",
    "    #print(trainIds)\n",
    "    \n",
    "    \n",
    "    trainValRoot = \"./data/images/train/\"\n",
    "    #testRoot = \"./data/images/test/\"\n",
    "    \n",
    "    trainValJson = \"./data/annotations/captions_train2014.json\"\n",
    "    #testJson = \"./data/annotations/captions_val2014.json\"\n",
    "    \n",
    "    \n",
    "    with open('./data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    transform = None\n",
    "    batch_size = 1\n",
    "    shuffle = True\n",
    "    num_workers = 1\n",
    "    \n",
    "    \n",
    "    trainDl = get_loader(trainValRoot, trainValJson, trainIds, vocab, \n",
    "                         transform=transforms.ToTensor(), batch_size=batch_size, \n",
    "                         shuffle=False, num_workers=1)\n",
    "#     valDl = get_loader(trainValRoot, trainValJson, valIds, vocab, \n",
    "#                          transform=None, batch_size=batch_size, \n",
    "#                          shuffle=shuffle, num_workers=1)\n",
    "#    testDl = get_loader(testRoot, testJson, testIds, vocab, \n",
    "#                         transform=None, batch_size=batch_size, \n",
    "#                         shuffle=shuffle, num_workers=1)\n",
    "    \n",
    "    encoded_feature_dim = 10\n",
    "    hidden_dim = 50\n",
    "    \n",
    "    encoder = Encoder(encoded_feature_dim)\n",
    "    decoder = Decoder(encoded_feature_dim, hidden_dim, vocab.idx)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    epochs = 100\n",
    "    trainEncoderDecoder(encoder, decoder, criterion, epochs, \n",
    "                        trainDl, trainDl, trainDl, \"LSTM\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds.index(509365)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainIds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
