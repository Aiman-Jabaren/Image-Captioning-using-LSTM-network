{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install bcolz Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bcolz in /home/pkhorram/.local/lib/python3.7/site-packages (1.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.7/site-packages (from bcolz) (1.16.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bcolz --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz \n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary of trained weights from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We will use “Wikipedia 2014 + Gigaword 5” which is the smallest file (“ glove.6B.zip”) with 822 MB.\n",
    "    It was trained on a corpus of 6 billion tokens and contains a vocabulary of 400 thousand tokens.\n",
    "    We pick the smallest one with words represented by vectors of dim 50 (“glove.6B.50d.txt”). \n",
    "\n",
    "'''\n",
    "\n",
    "if os.path.isdir('./GloVe'):\n",
    "    print('Glove Folder already created')\n",
    "else:\n",
    "    os.mkdir('GloVe')\n",
    "    num = 0\n",
    "    word_list = []\n",
    "    word_num = {}\n",
    "    vectors = bcolz.carray(np.zeros(1), rootdir='./GloVe/6B.50.dat', mode='w')\n",
    "\n",
    "    with open('./glove.6B.50d.txt', 'rb') as f:\n",
    "        for L in f:\n",
    "            line = L.decode().split()\n",
    "            word = line[0]\n",
    "            word_list.append(word)\n",
    "            word_num[word] = num\n",
    "            num = num + 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            vectors.append(vect)\n",
    "\n",
    "    vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir='./GloVe/6B.50.dat', mode='w')\n",
    "    vectors.flush()\n",
    "    pickle.dump(word_list, open('./GloVe/6B.50_words.pkl', 'wb'))\n",
    "    pickle.dump(word_num, open('./GloVe/6B.50_idx.pkl', 'wb'))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
