{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freshDecoder import *\n",
    "from encoder import *\n",
    "from fresh_data_loader import *\n",
    "import pickle\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import torchvision.transforms as tf\n",
    "import json\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_test(val_loader, encoder, decoder, criterion, maxSeqLen,\n",
    "             vocab, batch_size, use_gpu = True, calculate_bleu = True):\n",
    "\n",
    "    save_generated_imgs = True\n",
    "    #Evaluation Mode\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    \n",
    "    references = list()\n",
    "    hypotheses = list() \n",
    "   \n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        count    = 0\n",
    "        loss_avg = 0\n",
    "        bleu1_avg = 0\n",
    "        bleu4_avg = 0\n",
    "                \n",
    "        for i, (inputs, caps, allcaps) in enumerate(val_loader):\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Move to device, if available\n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)\n",
    "                caps = caps.to(device)\n",
    "\n",
    "                        \n",
    "            enc_out = encoder(inputs)\n",
    "            actual_lengths = allcaps\n",
    "            \n",
    "            \n",
    "            \n",
    "            temperature = 1\n",
    "            test_pred = decoder.generate_caption(enc_out, maxSeqLen, temperature)\n",
    "\n",
    "            test_pred_sample = test_pred[0].cpu().numpy()          \n",
    "        \n",
    "            #k = 0\n",
    "            if i <=10:\n",
    "                for b in range(0,inputs.shape[0]):\n",
    "                    caption = (\" \").join([vocab.idx2word[x.item()] for x in test_pred[b]])\n",
    "                    img = tf.ToPILImage()(inputs[b,:,:,:].cpu())\n",
    "                    plt.axis('off')\n",
    "                    plt.imshow(img)\n",
    "                    \n",
    "                    plt.show()\n",
    "                    print(\"Caption: \" + caption)\n",
    "                #if save_generated_imgs:\n",
    "                #    file = \"./generated_imgs/\" + \"test_im_\"+ str(b) \n",
    "                #    img.save(file + \".png\", \"PNG\")\n",
    "                    #if b >= 10:\n",
    "                    #    break\n",
    "                    #k+=1\n",
    "                    #with open(generated_imgs_filename, \"a\") as file:\n",
    "                        #file.write(\"writing! \" + \"train_epoch\" + str(epoch) + \"im_\"+ str(k) + \"\\n\")            \n",
    "                        #file.write(\"Caption: \" + caption +\"\\n \\n\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "\n",
    "            \n",
    "            #Build a list of the predicted sentences\n",
    "            # Convert word_ids to words\n",
    "            sampled_caption = []\n",
    "\n",
    "            for word_id in test_pred_sample:\n",
    "                word = vocab.idx2word[word_id]\n",
    "                sampled_caption.append(word)\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "            sentence = ' '.join(sampled_caption)\n",
    "            hypotheses.append(sampled_caption) \n",
    "\n",
    "            \n",
    "            #print('i: ', i)\n",
    "            #print('len(sampled_caption): ',len(sampled_caption))\n",
    "             \n",
    "            \n",
    "            decoder.resetHidden(inputs.shape[0])\n",
    "            outputs = decoder(caps, enc_out, actual_lengths)\n",
    "            \n",
    "            loss = criterion(outputs, Variable(caps.long()))\n",
    "            loss_avg += loss\n",
    "            count+=1\n",
    "            \n",
    "            #del outputs            \n",
    "            \n",
    "            #print('VAL: loss: ', loss)\n",
    "\n",
    "\n",
    "            caps_array = caps.cpu().numpy()  \n",
    "            # Convert word_ids to words\n",
    "            reference_caption = []\n",
    "            sampled_caption = []\n",
    "            \n",
    "            for word_id in caps_array[0]:\n",
    "                word = vocab.idx2word[word_id]\n",
    "                reference_caption.append(word)\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "            ref_sentence = ' '.join(reference_caption)\n",
    "            #if i % 500 == 0:\n",
    "                #print('ref_sentence: ', ref_sentence)\n",
    "                #print('len(ref_sentence): ',len(reference_caption))\n",
    "            references.append(reference_caption)   \n",
    "            #print('len(reference_caption): ',len(reference_caption))\n",
    "        \n",
    "            #print('len(references)', len(references))\n",
    "            #print('len(hypotheses)', len(hypotheses))\n",
    "            #print('references: ', references)\n",
    "            #print('hypotheses: ', hypotheses)\n",
    "      \n",
    "            del caps\n",
    "            del outputs            \n",
    "            \n",
    "            \n",
    "            #if i % 10 == 0:\n",
    "            #    break\n",
    "             \n",
    "        # Calculate BLEU-4 scores\n",
    "        if calculate_bleu:\n",
    "            #TODO\n",
    "            #print('len(references)',len(references))\n",
    "            #print('len(hypotheses)',len(hypotheses))\n",
    "            bleu4 = corpus_bleu(references, hypotheses)                \n",
    "            bleu1 = corpus_bleu(references, hypotheses,weights=(1.0, 0, 0, 0))\n",
    "            #bleu4 = corpus_bleu(reference_caption, sampled_caption)                \n",
    "            #bleu1 = corpus_bleu(reference_caption, sampled_caption,weights=(1.0, 0, 0, 0))\n",
    "            #print('bleu4: ', bleu4)        \n",
    "            #print('bleu1: ', bleu1)  \n",
    "#            bleu4_avg+=bleu4\n",
    "#            bleu1_avg+=bleu1\n",
    "                            \n",
    "                \n",
    "        loss_avg  = loss_avg/count\n",
    "        print('VAL: loss_avg: ', loss_avg)\n",
    "\n",
    "        if calculate_bleu:\n",
    "            \n",
    "            bleu4_avg = bleu4\n",
    "            bleu1_avg = bleu1 \n",
    "            \n",
    "            print('VAL: bleu4_avg: ', bleu4_avg)\n",
    "            print('VAL: bleu1_avg: ', bleu1_avg)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    return loss_avg, bleu1_avg, bleu4_avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    name = \"lstm\"\n",
    "\n",
    "    with open('TrainImageIds.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        trainIds = list(reader)[0]\n",
    "        \n",
    "    with open('TestImageIds.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        testIds = list(reader)[0]\n",
    "\n",
    "        \n",
    "    #if len(sys.argv) > 1:\n",
    "    #    name = sys.argv[1]\n",
    "        \n",
    "        \n",
    "    trainIds = [int(i) for i in trainIds]\n",
    "    testIds = [int(i) for i in testIds]\n",
    "    \n",
    "    # Will shuffle the trainIds incase of ordering in csv\n",
    "    random.shuffle(trainIds)\n",
    "    splitIdx = int(len(trainIds)/5)\n",
    "    \n",
    "    # Selecting 1/5 of training set as validation\n",
    "    valIds = trainIds[:splitIdx]\n",
    "    trainIds = trainIds[splitIdx:]\n",
    "    \n",
    "    \n",
    "    trainValRoot = \"./data/images/train/\"\n",
    "    testRoot = \"./data/images/test/\"\n",
    "    \n",
    "    trainValJson = \"./data/annotations/captions_train2014.json\"\n",
    "    testJson = \"./data/annotations/captions_val2014.json\"\n",
    "    \n",
    "    \n",
    "    with open('./data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    img_side_length = 256\n",
    "    transform = tf.Compose([\n",
    "        tf.Resize(img_side_length),\n",
    "        #tf.RandomCrop(img_side_length),\n",
    "        tf.CenterCrop(img_side_length),\n",
    "        tf.ToTensor(),\n",
    "    ])\n",
    "    batch_size = 10\n",
    "    shuffle = True\n",
    "    num_workers = 5\n",
    "    \n",
    "    \n",
    "    trainDl = get_loader(trainValRoot, trainValJson, trainIds, vocab, \n",
    "                         transform=transform, batch_size=batch_size, \n",
    "                         shuffle=shuffle, num_workers=num_workers)\n",
    "    valDl = get_loader(trainValRoot, trainValJson, valIds, vocab, \n",
    "                         transform=transform, batch_size=batch_size, \n",
    "                         shuffle=shuffle, num_workers=num_workers)\n",
    "    testDl = get_loader(testRoot, testJson, testIds, vocab, \n",
    "                        transform=transform, batch_size=batch_size, \n",
    "                        shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    encoded_feature_dim = 1024\n",
    "    maxSeqLen = 56\n",
    "    hidden_dim = 1500\n",
    "    depth = 1\n",
    "    \n",
    "    encoder = Encoder(encoded_feature_dim)\n",
    "    decoder = Decoder(encoded_feature_dim, hidden_dim, depth, vocab.idx, batch_size)    \n",
    "    \n",
    "    encoder = torch.load('weights_base/' + name + 'encoder_best')\n",
    "    decoder = torch.load('weights_base/' + name + 'decoder_best')    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    val_loss, val_bleu1, val_bleu4  = validate_test(testDl, encoder, decoder, criterion,maxSeqLen,\n",
    "                             vocab, batch_size, use_gpu= True, calculate_bleu = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
